proc.go/
    func newproc1(fn *funcval, callergp *g, callerpc uintptr) *g {}
    func malg(stacksize int32) *g {}

stack.go/
    func stackalloc(n uint32) stack {}

runtime2.go/
    p.runq
    p.runqhead
    p.runqtail

https://medium.com/a-journey-with-go/go-how-does-the-goroutine-stack-size-evolve-447fc02085e5
https://www.youtube.com/watch?v=n8_2y5E8N4Y
https://www.youtube.com/watch?v=jKcg3ze10Hk
http://www.ee.nmt.edu/~erives/308L_05/The_stack.pdf
https://www.redhat.com/en/blog/security-technologies-stack-smashing-protection-stackguard
https://www.researchgate.net/publication/234784207_StackGuard_Automatic_adaptive_detection_and_prevention_of_buffer-overflow_attacks

// is 0 on Linux
_StackSystem

// The minimum size of stack used by Go code
_StackMin = 2048

// The minimum stack size to allocate.
// The hackery here rounds FixedStack0 up to a power of 2.
_FixedStack0 = _StackMin + _StackSystem  // 2048
_FixedStack1 = _FixedStack0 - 1  // 2047
_FixedStack2 = _FixedStack1 | (_FixedStack1 >> 1) // 2047
_FixedStack3 = _FixedStack2 | (_FixedStack2 >> 2) // 2047
_FixedStack4 = _FixedStack3 | (_FixedStack3 >> 4) // 2047
_FixedStack5 = _FixedStack4 | (_FixedStack4 >> 8) // 2047
_FixedStack6 = _FixedStack5 | (_FixedStack5 >> 16) // 2047
_FixedStack  = _FixedStack6 + 1 // 2048

// Number of orders that get caching. Order 0 is FixedStack
// and each successive order is twice as large.
// We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks
// will be allocated directly.
// Since FixedStack is different on different systems, we
// must vary NumStackOrders to keep the same maximum cached size.
//   OS               | FixedStack | NumStackOrders
//   -----------------+------------+---------------
//   linux/darwin/bsd | 2KB        | 4
//   windows/32       | 4KB        | 3
//   windows/64       | 8KB        | 2
//   plan9            | 4KB        | 3
_NumStackOrders = 4 - goarch.PtrSize/4*goos.IsWindows - 1*goos.IsPlan9

// _FixedStack<<_NumStackOrders = 32768 // 32KB

// Per-P, per order stack segment cache size.
_StackCacheSize = 32 * 1024 // 32KB


_StackGuard = 928*sys.StackGuardMultiplier + _StackSystem // 928 bytes on linux


--------------------------------------------------------------------------------------------------

newproc()

// Create a new g running fn.
// Put it on the queue of g's waiting to run.
// The compiler turns a go statement into a call to this.
func newproc(fn *funcval) {}

Gets pointer to current g from r14 register.

Gets program counter (from register) to which the caller of the 'go' statement will return once 
the call completes.

On systemstack:
    Creates a new g prepared for being put onto a run queue (for details check out newproc1).

    Gets the P that the goroutine which is executing the 'go' statement is associated with.

    Puts the newly created goroutine into P's runnext slot. If another goroutine previously
    occupied this slot, it will be put onto the end of the local run queue or onto the global
    run queue if the local one is full. If the goroutine is going to be put onto the global
    run queue, then half of the local run queue (123 goroutines) will be put onto the global
    run queue as well.

    TODO!!!!!!!!!!!!!!!!!!


--------------------------------------------------------------------------------------------------


newproc1()

// Create a new g in state _Grunnable, starting at fn. callerpc is the
// address of the go statement that created this. The caller is responsible
// for adding the new g to the scheduler.
func newproc1(fn *funcval, callergp *g, callerpc uintptr) *g {}

Aqcuires current g, the g which has made the 'go' call.
TODO!!!!!!!!!!!!!!!!!!!! locks current P? (disable preemption because it 
                                           can be holding p in a local var)

Tries to get a cached goroutine object from the current P's gFree cache (this may 
involve asking for a batch of goroutines from the global cache).

If there are no cached goroutine objects available, a new goroutine object and its stack 
of 2048 bytes (2KB) will be allocated (check out malg() for more details).
The new goroutine will immediately transition from its default _Gidle status to _Gdead 
to mimic as if it came from the gFree cache and so that GC scanner doesn't look at 
uninitialized stack when the goroutine is added to the global list of goroutines (check out 
casgstatus for more details).
TODO!!!!!!!!!!!!!!!!!!!! add more details.
Adds the newly created goroutine to the global list of goroutines which is used by 
the garbage collector.

Sets the stack pointer sp variable to 32 bytes above the stacks starting address (technically
lower because stack grows from higher to lower memory). Is extra space in case of reads 
slightly beyond frame. Also sets spArg to sp. TODO!!!!!!!!!!!!!!!!!!!! how is this used?

Clear's the goroutine's sched struct field from any left over heap pointers 
(technicaly only the ctxt field can be a pointer to heap).
A goroutine's sched contains the following fields:
    - sp (stack pointer)
    - pc (program counter)
    - pointer to the owning goroutine
    - ctxt (it may be a heap-allocated funcval, ctxt is really a
            saved, live register, and we only ever exchange it between
            the real register and the gobuf)
            TODO!!!!!!!!!!!!!!!!!! what is it used for?
    - ret (return address)
    - lr TODO!!!!!!!!!!!!!!!! what is it used for?
    - bp (base pointer for framepointer-enabled architectures) TODO!!!!!! is x86 such an arch?

Sets goroutine's stack pointer in sched to the starting address of the stack.
Also store the stack pointer in the goroutine's 'stktopsp field, which is the expected 
sp at top of stack, to check in traceback.

TODO!!!!!!!!!!!!!!!!!!!!!!!!!!!!! not sure about this... and how will the PC be used?
Sets the program counter sched.pc to the address of the goexit function + 1 so that when the
top most user-defined function return the program execution transfers straight to goexit.
(FuncPCABI0 returns the entry PC of the function f. FuncPCABI0 is implemented 
as a compile intrinsic)
(The top-most function running on a goroutine returns to goexit+PCQuantum)
goexit calls goexit1 which calls goexit0 which never returns (goexit continuation on g0)

Sets sched.g to the new *goroutine owning sched.

Adjusts sched fields as if the goroutine executed a call to fn and then stopped before 
the first instruction in fn.

This is done by placing the program counter onto the stack, which currently points to the
goexit function, so that when RET is executed it jumps to goexit. 
The above procedure is accomplished by decrementing sched.sp by 8 bytes and setting the memory
address that the modified sp points to to sched.pc.

The g.sched.sp now points 8 bytes lower in memory than before.

Sets the g.sched.pc program counter to point to the first instruction of fn.

Sets g.sched.ctxt to hold the pointer to funcval passed in into this method.

Sets g.gopc to the caller's pc (pc of go statement that created this goroutine) - pc after the 'go'
statement completes.

Sets g.startpc to the address of the function passed in

TODO!!!!!!!!!!!!!!!!!! 
    if isSystemGoroutine(newg, false) { atomic.Xadd(&sched.ngsys, +1)} else {// Only user goroutines inherit pprof labels...}

TODO!!!!!!!!!!!!!!!!!! newg.trackingSeq, newg.tracking

Transitions the goroutine to _Grunnable status.

TODO!!!!!!!!!!!!!!!!!! gcController.addScannableStack(_p_, int64(newg.stack.hi-newg.stack.lo))

TODO!!!!!!!!!!!!!!!!!! if _p_.goidcache == _p_.goidcacheend {}

TODO!!!!!!!!!!!!!!!!!!!!!!!! if raceenabled {}

TODO!!!!!!!!!!!!!!!!!!!!!!!! if trace.enabled {}

Releases M TODO!!!!!!!!!!!!!!!!

Returns the new initialized goroutine.


--------------------------------------------------------------------------------------------------

RUNQUEUES

Local run queue:
    Is a circuilar queue bounded by 256 elements (goroutine pointers).
    The queue consists of p.runqhead, p.runqtail and p.runq array of size 256 ([256]guintptr).

Global run queue:
    Is a field in the globally accessible sched variable of type schedt struct.
    Is a doubly-linked list (dequeue) of goroutines protected by sched.lock.
    The goroutines within the dequeue are linked through their schedlink fields.
    The sched's runqsize field holds the current size of the dequeue.


--------------------------------------------------------------------------------------------------


runqput()

// runqput tries to put g on the local runnable queue.
// If next is false, runqput adds g to the tail of the runnable queue.
// If next is true, runqput puts g in the _p_.runnext slot.
// If the run queue is full, runnext puts g on the global queue.
// Executed only by the owner P.
func runqput(_p_ *p, gp *g, next bool) {}

if randomizeScheduler TODO!!!!!!!!!!!!!!!!!!

next is true:
    - When is scheduled as a result of a user 'go' statement
    - TODO!!!!!!!!!!!!!!!! more points to add

If next is true and there was no goroutine previously stored in p's runnext slot, then
g is atomically stored in it and the function returns.

If some other goroutine was previously stored in p's runnext slot or next is false:
    If next is true, the old goroutine is retrieved from p's runnext slot before the new one will be
    stored in there as the previously stored one will be added to the tail of the queue.
    If next is false the new goroutine will be added to the tail of the queue.

    Tries to insert the goroutine at the tail of the local run queue circuilar buffer, if inserted
    successfully this method returns.

    If the local run queue is full (has 256 elements) then the goroutine together with 123 goroutines 
    from the local run queue will be put onto the global run queue protected by sched.lock.
    Basically after this the local run queue will be half empty.

--------------------------------------------------------------------------------------------------


runqputslow()

// Put g and a batch of work from local runnable queue on global queue.
// Executed only by the owner P.
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {}

Puts a half (123) of the goroutines from the local runnable queue plus the passed in goroutine
onto the global runnable queue (124 altogether).


--------------------------------------------------------------------------------------------------


// If asked to move to or from a Gscanstatus this will throw. Use the castogscanstatus
// and casfrom_Gscanstatus instead.
// casgstatus will loop if the g->atomicstatus is in a Gscan status until the routine that
// put it in the Gscan state is finished.
//
//go:nosplit
func casgstatus(gp *g, oldval, newval uint32) {}

_Gidle = 0 (means this goroutine was just allocated and has not yet been initialized)

_Grunnable = 1 (means this goroutine is on a run queue. It is not currently executing 
                user code. The stack is not owned)
                
_Grunning = 2 (means this goroutine may execute user code. The stack is owned by 
              this goroutine. It is not on a run queue. It is assigned an M 
              and a P (g.m and g.m.p are valid))

_Gsyscall = 3 (means this goroutine is executing a system call. It is 
              not executing user code. The stack is owned by this goroutine. It is not 
              on a run queue. It is assigned an M)

_Gwaiting = 4 (means this goroutine is blocked in the runtime. It is not executing 
              user code. It is not on a run queue, but should be recorded somewhere 
              (e.g., a channel wait queue) so it can be ready()d when necessary.
              The stack is not owned *except* that a channel operation may read or
              write parts of the stack under the appropriate channel lock.
              Otherwise, it is not safe to access the stack after a goroutine enters 
              _Gwaiting (e.g., it may get moved))

_Gdead = 6 (means this goroutine is currently unused. It may be just exited, 
           on a free list, or just being initialized. It is not executing user code. 
           It may or may not have a stack allocated. The G and its stack (if any) 
           are owned by the M that is exiting the G or that obtained the G from the free
           list)

_Gcopystack = 8 (means this goroutine's stack is being moved. It is not executing 
                 user code and is not on a run queue. The stack is owned by the goroutine
                 that put it in _Gcopystack)

_Gpreempted = 9 (means this goroutine stopped itself for a suspendG preemption.
                 It is like _Gwaiting, but nothing is yet responsible for ready()ing it.
                 Some suspendG must CAS the status to _Gwaiting to take responsibility
                 for ready()ing this G)

_Gscan combined with one of the above states other than
_Grunning indicates that GC is scanning the stack. The
goroutine is not executing user code and the stack is owned
by the goroutine that set the _Gscan bit.

_Gscanrunning is different: it is used to briefly block
state transitions while GC signals the G to scan its own
stack. This is otherwise like _Grunning.

atomicstatus&~Gscan gives the state the goroutine will
return to when the scan completes.

_Gscan = 4096
_Gscanrunnable = 4097
_Gscanrunning = 4098
_Gscansyscall = 4099
_Gscanwaiting = 4100
_Gscanpreempted = 4105

Throws if is asked to move to or from _Gscan status or the old and destination
statuses are equal.

If in the meantime this goroutine transitioned to _Gscan, the act of acquire-releasing 
the runtime's lockRankGscan mutex will block until the goroutine transitions back to 
to its previous status.

If after the lock was released this goroutine transitioned to _Gscan again, loops until the 
goroutine transitions back to its previous status.
The loop includes spinning and yielding control to the OS, causing thread rescheduling.

Sets the goroutine's status through a CAS operation.

Additionally may feed a histogram metric every 8th time this goroutine transitions 
from _Grunnable to _Grunning  with the value of "how much has this goroutine spent 
in a run queue until it was actually run".


--------------------------------------------------------------------------------------------------


func malg(stacksize int32) *g {}

// round x up to a power of 2.
// _StackSystem is 0 on linux
stacksize = round2(_StackSystem + stacksize) 

Can only be called by a g0 scheduling goroutine.

Allocates a pointer to a new uninitialized g and rounds requested stack size to be a power of 2.

Calls stackalloc to allocate a new stack of the requested size (stackalloc runs on system stack).

If the requested stack size is less than 32Kb it will be allocated from the current
P's mcache which contains a list of free stacks for each stack order (2KB, 4KB, 8KB, and 16KB stacks).

If g0 is running without a P, which can happen in the guts of exitsyscall or procresize,
the stack is allocated from a globally accessible list of free stacks.

If the requested stack size is 32Kb or bigger a dedicated span of the required number of
pages to fit the stack is allocated.

The stack's starting and ending addresses will be recorerded in hi and lo fields of the 
stack object respectively (checkout stackalloc for mor details).
The stack spans from hi to lo (from higher to lower addresses)

Records the address of the stackguard0 which is 928 bytes higher in memory 
than the stack's lo address and is used to check for stack overflow aka if the stack 
needs to be grown. 

Return a pointer to the goroutine object.


--------------------------------------------------------------------------------------------------


// stackalloc allocates an n byte stack.
//
// stackalloc must run on the system stack because it uses per-P
// resources and must not split the stack.
//
//go:systemstack
func stackalloc(n uint32) stack {}

Can only be called by g0 goroutine which performs scheduling.

The requested stack size must be a power of 2.

If the requested stack size is less than 32Kb it will be allocated from the current
P's mcache for small objects which contains a list of free stacks for each stack order
(2KB, 4KB, 8KB, and 16KB stacks).
If g0 is running without a P, which can happen in the guts of exitsyscall or procresize,
the stack is allocated from a globally accessible list of free stacks 
(this pool also contains a list for each stack order).

If the local stack cache is empty for the requested stack order it's refilled from
the global stack pool. In this case the global pool is required to prevent unlimited growth 
of per-thread caches.

A stack is popped from the head of the global or local stack linked list and its
starting (hi) and ending (lo) addresses will be encapsulated within the returned stack object. 

If the requested stack size is 32Kb or bigger a dedicated span of required number of
pages to fit the stack is allocated and the span's starting (hi) and ending (lo) addresses
will be encapsulated within the returned stack object.


--------------------------------------------------------------------------------------------------


startm()

// Schedules some M to run the p (creates an M if necessary).
// If p==nil, tries to get an idle P, if no idle P's does nothing.
// May run with m.p==nil, so write barriers are not allowed.
// If spinning is set, the caller has incremented nmspinning and startm will
// either decrement nmspinning or set m.spinning in the newly started M.
//
// Callers passing a non-nil P must call from a non-preemptible context. See
// comment on acquirem below.
//
// Must not have write barriers because this may be called without a P.
//
//go:nowritebarrierrec
func startm(_p_ *p, spinning bool) {}

Disable preemption.
Every owned P must have an owner that will eventually stop it in the
event of a GC stop request. startm takes transient ownership of a P
(either from argument or pidleget below) and transfers ownership to
a started M, which will be responsible for performing the stop.

Preemption must be disabled during this transient ownership,
otherwise the P this is running on may enter GC stop while still
holding the transient P, leaving that P in limbo and deadlocking the STW.

Callers passing a non-nil P must already be in non-preemptible
context, otherwise such preemption could occur on function entry to
startm. Callers passing a nil P may be preemptible, so we must
disable preemption before acquiring a P from pidleget below.

TODO!!!!!!!!!!!!!!!!!! mp := acquirem()
                       lock(&sched.lock)

TODO!!!!!!!!!!!!!!!!!! _p_, _ = pidleget(0)

TODO!!!!!!!!!!!!!!!!!! mget


--------------------------------------------------------------------------------------------------

// Put mp on midle list.
// sched.lock must be held.
// May run during STW, so write barriers are not allowed.
//
//go:nowritebarrierrec
func mput(mp *m) {}

TODO!!!!!!!!!!!!!!!!!!


--------------------------------------------------------------------------------------------------


mget()

// Try to get an m from midle list.
// sched.lock must be held.
// May run during STW, so write barriers are not allowed.
//
//go:nowritebarrierrec
func mget() *m {}

TODO!!!!!!!!!!!!!!!!!! when is an M added to the idle list?


--------------------------------------------------------------------------------------------------


newm()

// Create a new m. It will start off with a call to fn, or else the scheduler.
// fn needs to be static and not a heap allocated closure.
// May run with m.p==nil, so write barriers are not allowed.
//
// id is optional pre-allocated m ID. Omit by passing -1.
//
//go:nowritebarrierrec
func newm(fn func(), _p_ *p, id int64) {}

TODO!!!!!!!!!!!!!!!!!! when and by whom is this method called?

Allocm adds a new M to allm, but they do not start until created by
the OS in newm1 or the template thread.

TODO!!!!!!!!!!!!!!!!!! doAllThreadsSyscall requires that every M in allm will eventually
                       start and be signal-able, even with a STW.

TODO!!!!!!!!!!!!!!!!!! Disable preemption here until we start the thread to ensure that
                       newm is not preempted between allocm and starting the new thread,
                       ensuring that anything added to allm is guaranteed to eventually
                       start.

Allocates a new M object and sets it's .mstartfn to the passed in function,
which, if not nil, will be executed after the new OS thread starts. 
Allocates a new signal-handling goroutine with a 32KB stack and stores a pointer
to it in M's gsignal field (check out malg() for more details)
Allocates an 8KB g0 scheduling goroutine (check out malg() for more details).
TODO!!!!!!!!!!!!!!!!!! g0 system stack?
Atomically stores the M in allm global linked list, which contains all Ms.

TODO!!!!!!!!!!!!!!!!!! sets M's .sigmask to initSigmask == 0

TODO!!!!!!!!!!!!!!!!!! We're on a locked M 


--------------------------------------------------------------------------------------------------


allocm()

// Allocate a new m unassociated with any thread.
// Can use p for allocation context if needed.
// fn is recorded as the new m's m.mstartfn.
// id is optional pre-allocated m ID. Omit by passing -1.
//
// This function is allowed to have write barriers even if the caller
// isn't because it borrows _p_.
//
//go:yeswritebarrierrec
func allocm(_p_ *p, fn func(), id int64) *m {}

TODO!!!!!!!!!!!!!!!!!! when and by whom is this method called?

TODO!!!!!!!!!!!!!!!!!! _p_

TODO!!!!!!!!!!!!!!!!!! id

TODO!!!!!!!!!!!!!!!!!! usually runs from systemstack (g0)?

TODO!!!!!!!!!!!!!!!!!! allocmLock.rlock()

TODO!!!!!!!!!!!!!!!!!! acquirem()

TODO!!!!!!!!!!!!!!!!!! getg() => if _g_.m.p == 0 => acquirep(_p_)

TODO!!!!!!!!!!!!!!!!!! if sched.freem != nil

Allocates a new M object and sets it's .mstartfn to the passed in function,
which, if not nil, will be executed after the new OS thread starts. 
TODO!!!!!!!!!!!!!!!!!! details about setup before .mstartfn is run

TODO!!!!!!!!!!!!!!!!!! mcommoninit => all code except mpreinit() and storing M in allm global linked list

Allocates a new goroutine with a 32KB stack and assigns it to M's gsignal signal-handling g.
Check out malg() for more details on goroutine allocation (doesn't actual add the g to any run queue).
Also stores a reference to this M in the signal-handling g.
TODO!!!!!!!!!!!!!!!!!! how is the signal-handling g used?

TODO!!!!!!!!!!!!!!!!!! // Add to allm so garbage collector doesn't free g->m
                       // when it is just in a register or thread-local storage.
                       mp.alllink = allm

Atomically stores the M in the allm global linked list, which contains all Ms.

Allocates an 8KB g0 goroutine (check out malg() for more details).

Returns a pointer to the new M object.


--------------------------------------------------------------------------------------------------


newm1()

func newm1(mp *m) {}

TODO!!!!!!!!!!!!!!!!!!






==================================================================================================
--------------------------------------------------------------------------------------------------
---------------------------------------os_linux.go------------------------------------------------
--------------------------------------------------------------------------------------------------
==================================================================================================


--------------------------------------------------------------------------------------------------


newosproc()

// May run with m.p==nil, so write barriers are not allowed.
//
//go:nowritebarrier
func newosproc(mp *m) {}

TODO!!!!!!!!!!!!!!!!!!

Uses new M's g0 stack starting (high) address as the stack argument to the clone
syscall. This means that the new OS thread will use the stack allocated for g0 (8192 Kb).

Clone flags used:
    CLONE_THREAD (since Linux 2.4.0)
        If CLONE_THREAD is set, the child is placed in the same
        thread group as the calling process.  To make the
        remainder of the discussion of CLONE_THREAD more readable,
        the term "thread" is used to refer to the processes within
        a thread group.
        Thread groups were a feature added in Linux 2.4 to support
        the POSIX threads notion of a set of threads that share a
        single PID.  Internally, this shared PID is the so-called
        thread group identifier (TGID) for the thread group.
        Since Linux 2.4, calls to getpid(2) return the TGID of the
        caller.
        The threads within a group can be distinguished by their
        (system-wide) unique thread IDs (TID).  A new thread's TID
        is available as the function result returned to the
        caller, and a thread can obtain its own TID using
        gettid(2).
        A new thread created with CLONE_THREAD has the same parent
        process as the process that made the clone call (i.e.,
        like CLONE_PARENT), so that calls to getppid(2) return the
        same value for all of the threads in a thread group.  When
        a CLONE_THREAD thread terminates, the thread that created
        it is not sent a SIGCHLD (or other termination) signal;
        nor can the status of such a thread be obtained using
        wait(2).  (The thread is said to be detached.)
        After all of the threads in a thread group terminate the
        parent process of the thread group is sent a SIGCHLD (or
        other termination) signal.
        If any of the threads in a thread group performs an
        execve(2), then all threads other than the thread group
        leader are terminated, and the new program is executed in
        the thread group leader.
        If one of the threads in a thread group creates a child
        using fork(2), then any thread in the group can wait(2)
        for that child.
        Since Linux 2.5.35, the flags mask must also include
        CLONE_SIGHAND if CLONE_THREAD is specified (and note that,
        since Linux 2.6.0, CLONE_SIGHAND also requires CLONE_VM to
        be included).
        Signal dispositions and actions are process-wide: if an
        unhandled signal is delivered to a thread, then it will
        affect (terminate, stop, continue, be ignored in) all
        members of the thread group.
        Each thread has its own signal mask, as set by
        sigprocmask(2).

    CLONE_VM (since Linux 2.0)
        If CLONE_VM is set, the calling process and the child
        process run in the same memory space.  In particular,
        memory writes performed by the calling process or by the
        child process are also visible in the other process.
        Moreover, any memory mapping or unmapping performed with
        mmap(2) or munmap(2) by the child or calling process also
        affects the other process.

    CLONE_FS (since Linux 2.0)
        If CLONE_FS is set, the caller and the child process (thread) share
        the same filesystem information.  This includes the root
        of the filesystem, the current working directory, and the
        umask.  Any call to chroot(2), chdir(2), or umask(2)
        performed by the calling process or the child process also
        affects the other process.

    CLONE_FILES (since Linux 2.0)
        If CLONE_FILES is set, the calling process and the child
        process share the same file descriptor table.  Any file
        descriptor created by the calling process or by the child
        process is also valid in the other process.  Similarly, if
        one of the processes closes a file descriptor, or changes
        its associated flags (using the fcntl(2) F_SETFD
        operation), the other process is also affected.  If a
        process sharing a file descriptor table calls execve(2),
        its file descriptor table is duplicated (unshared).

    CLONE_SIGHAND (since Linux 2.0)
        If CLONE_SIGHAND is set, the calling process and the child
        process share the same table of signal handlers.  If the
        calling process or child process calls sigaction(2) to
        change the behavior associated with a signal, the behavior
        is changed in the other process as well.  However, the
        calling process and child processes still have distinct
        signal masks and sets of pending signals.  So, one of them
        may block or unblock signals using sigprocmask(2) without
        affecting the other process.
        Since Linux 2.6.0, the flags mask must also include
        CLONE_VM if CLONE_SIGHAND is specified.

    CLONE_SYSVSEM (since Linux 2.5.10)
        If CLONE_SYSVSEM is set, then the child and the calling
        process share a single list of System V semaphore
        adjustment (semadj) values (see semop(2)).  In this case,
        the shared list accumulates semadj values across all
        processes sharing the list, and semaphore adjustments are
        performed only when the last process that is sharing the
        list terminates (or ceases sharing the list using
        unshare(2)).





==================================================================================================
--------------------------------------------------------------------------------------------------
-----------------------------------sys_linux_amd64.s----------------------------------------------
--------------------------------------------------------------------------------------------------
==================================================================================================


--------------------------------------------------------------------------------------------------


clone()
// int32 clone(int32 flags, void *stk, M *mp, G *gp, void (*fn)(void));
TEXT runtime·clone(SB),NOSPLIT,$0



Calls the clone syscall with 

long clone(unsigned long flags, void *stack, int *parent_tid, int *child_tid, unsigned long tls);
FS (16 bits) - general purpose segment pointer 

CLONE_FILES (since Linux 2.0)
    If CLONE_FILES is set, the calling process and the child
    process share the same file descriptor table.  Any file
    descriptor created by the calling process or by the child
    process is also valid in the other process.  Similarly, if
    one of the processes closes a file descriptor, or changes
    its associated flags (using the fcntl(2) F_SETFD
    operation), the other process is also affected.  If a
    process sharing a file descriptor table calls execve(2),
    its file descriptor table is duplicated (unshared).

    If CLONE_FILES is not set, the child process inherits a
    copy of all file descriptors opened in the calling process
    at the time of the clone call.  Subsequent operations that
    open or close file descriptors, or change file descriptor
    flags, performed by either the calling process or the
    child process do not affect the other process.  Note,
    however, that the duplicated file descriptors in the child
    refer to the same open file descriptions as the
    corresponding file descriptors in the calling process, and
    thus share file offsets and file status flags (see
    open(2)).


CLONE_FS (since Linux 2.0)
    If CLONE_FS is set, the caller and the child process share
    the same filesystem information.  This includes the root
    of the filesystem, the current working directory, and the
    umask.  Any call to chroot(2), chdir(2), or umask(2)
    performed by the calling process or the child process also
    affects the other process.

    If CLONE_FS is not set, the child process works on a copy
    of the filesystem information of the calling process at
    the time of the clone call.  Calls to chroot(2), chdir(2),
    or umask(2) performed later by one of the processes do not
    affect the other process.

    CLONE_SIGHAND (since Linux 2.0)
    If CLONE_SIGHAND is set, the calling process and the child
    process share the same table of signal handlers.  If the
    calling process or child process calls sigaction(2) to
    change the behavior associated with a signal, the behavior
    is changed in the other process as well.  However, the
    calling process and child processes still have distinct
    signal masks and sets of pending signals.  So, one of them
    may block or unblock signals using sigprocmask(2) without
    affecting the other process.

    If CLONE_SIGHAND is not set, the child process inherits a
    copy of the signal handlers of the calling process at the
    time of the clone call.  Calls to sigaction(2) performed
    later by one of the processes have no effect on the other
    process.

    Since Linux 2.6.0, the flags mask must also include
    CLONE_VM if CLONE_SIGHAND is specified.

CLONE_THREAD (since Linux 2.4.0)
    If CLONE_THREAD is set, the child is placed in the same
    thread group as the calling process.  To make the
    remainder of the discussion of CLONE_THREAD more readable,
    the term "thread" is used to refer to the processes within
    a thread group.

    Thread groups were a feature added in Linux 2.4 to support
    the POSIX threads notion of a set of threads that share a
    single PID.  Internally, this shared PID is the so-called
    thread group identifier (TGID) for the thread group.
    Since Linux 2.4, calls to getpid(2) return the TGID of the
    caller.

    The threads within a group can be distinguished by their
    (system-wide) unique thread IDs (TID).  A new thread's TID
    is available as the function result returned to the
    caller, and a thread can obtain its own TID using
    gettid(2).

    When a clone call is made without specifying CLONE_THREAD,
    then the resulting thread is placed in a new thread group
    whose TGID is the same as the thread's TID.  This thread
    is the leader of the new thread group.

    A new thread created with CLONE_THREAD has the same parent
    process as the process that made the clone call (i.e.,
    like CLONE_PARENT), so that calls to getppid(2) return the
    same value for all of the threads in a thread group.  When
    a CLONE_THREAD thread terminates, the thread that created
    it is not sent a SIGCHLD (or other termination) signal;
    nor can the status of such a thread be obtained using
    wait(2).  (The thread is said to be detached.)

    After all of the threads in a thread group terminate the
    parent process of the thread group is sent a SIGCHLD (or
    other termination) signal.

    If any of the threads in a thread group performs an
    execve(2), then all threads other than the thread group
    leader are terminated, and the new program is executed in
    the thread group leader.

    If one of the threads in a thread group creates a child
    using fork(2), then any thread in the group can wait(2)
    for that child.

    Since Linux 2.5.35, the flags mask must also include
    CLONE_SIGHAND if CLONE_THREAD is specified (and note that,
    since Linux 2.6.0, CLONE_SIGHAND also requires CLONE_VM to
    be included).

    Signal dispositions and actions are process-wide: if an
    unhandled signal is delivered to a thread, then it will
    affect (terminate, stop, continue, be ignored in) all
    members of the thread group.

    Each thread has its own signal mask, as set by
    sigprocmask(2).

    A signal may be process-directed or thread-directed.  A
    process-directed signal is targeted at a thread group
    (i.e., a TGID), and is delivered to an arbitrarily
    selected thread from among those that are not blocking the
    signal.  A signal may be process-directed because it was
    generated by the kernel for reasons other than a hardware
    exception, or because it was sent using kill(2) or
    sigqueue(3).  A thread-directed signal is targeted at
    (i.e., delivered to) a specific thread.  A signal may be
    thread directed because it was sent using tgkill(2) or
    pthread_sigqueue(3), or because the thread executed a
    machine language instruction that triggered a hardware
    exception (e.g., invalid memory access triggering SIGSEGV
    or a floating-point exception triggering SIGFPE).

    A call to sigpending(2) returns a signal set that is the
    union of the pending process-directed signals and the
    signals that are pending for the calling thread.

    If a process-directed signal is delivered to a thread
    group, and the thread group has installed a handler for
    the signal, then the handler is invoked in exactly one,
    arbitrarily selected member of the thread group that has
    not blocked the signal.  If multiple threads in a group
    are waiting to accept the same signal using
    sigwaitinfo(2), the kernel will arbitrarily select one of
    these threads to receive the signal.

CLONE_SETTLS (since Linux 2.5.32)
    The TLS (Thread Local Storage) descriptor is set to tls.

    The interpretation of tls and the resulting effect is
    architecture dependent.  On x86, tls is interpreted as a
    struct user_desc * (see set_thread_area(2)).  On x86-64 it
    is the new value to be set for the %fs base register (see
    the ARCH_SET_FS argument to arch_prctl(2)).  On
    architectures with a dedicated TLS register, it is the new
    value of that register.

    Use of this flag requires detailed knowledge and generally
    it should not be used except in libraries implementing
    threading.

ARCH_SET_FS
    Set the 64-bit base for the FS register to addr.


The runtime pointer to the <code>g</code> structure is maintained
through the value of an otherwise unused (as far as Go is concerned) register in the MMU.
In the runtime package, assembly code can include <code>go_tls.h</code>, which defines
an OS- and architecture-dependent macro <code>get_tls</code> for accessing this register.
The <code>get_tls</code> macro takes one argument, which is the register to load the
<code>g</code> pointer into.

For example, the sequence to load <code>g</code> and <code>m</code>
using <code>CX</code> looks like this:
#include "go_tls.h"
#include "go_asm.h"
...
get_tls(CX)
MOVL	g(CX), AX     // Move g into AX.
MOVL	g_m(AX), BX   // Move g.m into BX.

#ifdef GOARCH_amd64
#define	get_tls(r)	MOVQ TLS, r
#define	g(r)	0(r)(TLS*1)
#endif

https://stackoverflow.com/questions/6611346/how-are-the-fs-gs-registers-used-in-linux-amd64
